---
knit: "bookdown::preview_chapter"
---

# Parallel Execution

R supports several levels of parallel execution, starting with executing code on multiple cores,
and going up to executing code in massively parallel Hadoop platforms.  Since R Version 2.14.0
parallel has provided support for parallel computation through forking (c.f.multicore) and sockets
(c.f.snow).

To illustrate parallel computation we will build rpart decision trees in parallel.
The required packages for this module include:

```{r}
library(parallel)
library(snow)
library(rpart)
```

## Getting Started
Let us use would be using the Weather data set from the rattle package in this chapter. Using the dataset we would be predicting the amount of rainfall for the next day. 

```{r,message=FALSE}
library(rattle)
ds <- weatherAUS
target <- "RainTomorrow"              #Defining the target variable
risk <- "RISK_MM"                     #Defining the risk
ds[target] <- as.factor(ds[[target]]) #Converting the target to factor
```

<h4> Data Preparation </h4>

```{r}
vars <- colnames(ds)
ignore <- vars[c(1, 2, if (exists("risk")) which(risk==vars))] #Ignoring the risk
vars <- setdiff(vars, ignore)
inputs <- setdiff(vars, target)                                #Defining the input variables
form <- paste(target,paste(inputs,                             #Defining the formula
                  collapse = " + "),sep = " ~ ")

nobs <- nrow(ds)                                               #Number of records
train <- sample(nobs, 0.7*nobs)                                #Sampling for train dataset
test <- setdiff(seq_len(nobs), train)                          #Test dataset
```

## Begin Parallel

The parallel package provides functions to distribute the computation across multiple cores and servers.
We first determine the number of cores available on the computer we are processing our data on

```{r}
cores <- detectCores()
cores
```

For the purpose of model building, we would be using wsrpart() function to build decision trees based on random subset of the data. 

In order to build models in parallel we will be using the function mcparallel(). This command forks (can be used only on Windows) the current process to build a tree 

```{r}
jobs <- lapply(1:cores,
               function(x) mcparallel(wsrpart(form,ds[train,vars], ntrees=1),
                                              name=sprintf("dt%02d",x)))
```

<h4> Collect Results </h4>

```{r}
system.time(model <- mccollect(jobs,wait=TRUE))
```
The decision tree will be available in the resulting list.
```{r}
length(model)
model[[1]][[1]]
```

## Parallel Processes Through Local Sockets

Before we proceed to run parallel processes over a network of workers (remote servers) we will
do the same, but have a single node cluster (the current server).  We use makeCluster() from
parallel to do this.

We begin with a simple example by creating a cluster of as many nodes as there are cores on the localhost.
```{r,error=T}
cl <- makeCluster(getOption("cl.cores", 2))
cl
```
Now we ask each node of the cluster to do something. In this case we get the addition function
with the additional argument 3.  The 1:2 are the arguments passed to each node, so that node 1
gets 1 and node 2 gets 2.
```{r,error=T}
clusterApply(cl,1:2,get("+"),3)
```


